{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis for Financial News using Naive Bayes and SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis\n",
        "**Sentiment analysis**, sometimes called **opinion mining** or **polarity detection**, refers to the **set of algorithms** and **techniques** that are used to **extract the polarity of a given document**; that is, it determines whether the **sentiment** of a **document** is \n",
        "\n",
        "* **Positive**, \n",
        "* **Negative**,\n",
        "* **Neutral**. \n",
        "\n",
        "**Sentiment analysis** is **gaining popularity** in the **industry** as it **allows organizations**\n",
        "to **mine opinions** of a **large group of users** or **potential customers** in a **cost-efficient\n",
        "way**. **Sentiment analysis** is now **used extensively**, among others in \n",
        "\n",
        "* **Advertisement campaigns**, \n",
        "* **Political campaigns**, \n",
        "* **Stock analysis**. "
      ],
      "metadata": {
        "id": "1bUVWR3sjS0A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Financial Phrase Bank dataset\n",
        "**[FinancialPhraseBank](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news) dataset** contains the **sentiments** for **financial news headlines** from the **perspective of a retail investor**. It contains **two columns**:\n",
        "* **Sentiment**\n",
        "* **News Headline**. "
      ],
      "metadata": {
        "id": "EzKS2_oNjncU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from spacy.tokens.doc import Doc\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_selection import chi2, SelectKBest\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from typing import Generator"
      ],
      "metadata": {
        "id": "dTN6qzQsc2Pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e86cdc-57f6-4c4c-8e18-efa391f6d2b0",
        "id": "H04ZDC6taC4X"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_csv = '/content/drive/MyDrive/Financial News dataset/financial_news.csv'"
      ],
      "metadata": {
        "id": "aialEyaac3mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df = pd.read_csv(\n",
        "    filepath_or_buffer=news_csv, \n",
        "    sep=';', \n",
        "    encoding='unicode_escape', \n",
        "    engine='python', \n",
        "    header=None,\n",
        "    usecols=[0, 1], \n",
        "    names=['Sentiment', 'News']\n",
        ")"
      ],
      "metadata": {
        "id": "agE4_iiFeN4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "22qjhxC5gRaJ",
        "outputId": "b33b24ef-8725-4aa0-93d9-5b29d8895c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Sentiment                                               News\n",
              "0   neutral  According to Gran , the company has no plans t...\n",
              "1   neutral  Technopolis plans to develop in stages an area...\n",
              "2  negative  The international electronic industry company ...\n",
              "3  positive  With the new production plant the company woul...\n",
              "4  positive  According to the company 's updated strategy f..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2f0e4d0e-602a-445f-a9e0-56e55e619c50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>News</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>According to Gran , the company has no plans t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>neutral</td>\n",
              "      <td>Technopolis plans to develop in stages an area...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>negative</td>\n",
              "      <td>The international electronic industry company ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>positive</td>\n",
              "      <td>With the new production plant the company woul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>positive</td>\n",
              "      <td>According to the company 's updated strategy f...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2f0e4d0e-602a-445f-a9e0-56e55e619c50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2f0e4d0e-602a-445f-a9e0-56e55e619c50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2f0e4d0e-602a-445f-a9e0-56e55e619c50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZph9He8hf1E",
        "outputId": "af2a2e4a-d1c3-4575-f423-17f8ef9f9eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4846 entries, 0 to 4845\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Sentiment  4846 non-null   object\n",
            " 1   News       4846 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 75.8+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = news_df['News']\n",
        "y = news_df['Sentiment']"
      ],
      "metadata": {
        "id": "y6QXaMCMlXIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.replace(\n",
        "    to_replace={'negative': -1, 'neutral': 0, 'positive': 1}, \n",
        "    inplace=True\n",
        ")"
      ],
      "metadata": {
        "id": "H4founYelqla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=17)"
      ],
      "metadata": {
        "id": "HV7_BKsGwySU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing pipeline\n",
        "There are **three transformations** included in the **pipeline**:\n",
        "* **CountVectorizer** is used to **tokenize sentences** to **lower-case words**, **stopwords are removed**, and output is **vectorized**.\n",
        "* **Chi2Score** selects **top k features** related to the **target** based on **ChiSquare test statistics**\n",
        "* **TfidfTransformer** transforms the **vector of selected k-features** to **TF-IDF representation**."
      ],
      "metadata": {
        "id": "-ZdfdOnXtdGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpacyPreprocessor(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.nlp = spacy.load(\n",
        "      name='en_core_web_sm', \n",
        "      disable=[\"parser\", \"ner\"]\n",
        "    )\n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, y=None) -> list:\n",
        "      return [\n",
        "        ' '.join(SpacyPreprocessor.process_document(doc)) \n",
        "        for doc in self.nlp.pipe(X)\n",
        "      ]\n",
        "\n",
        "  @staticmethod\n",
        "  def process_document(doc: Doc) -> Generator[str, None, None]:\n",
        "    # Tokenize document\n",
        "    for token in doc:\n",
        "      # Remove non-alphanumeric tokens\n",
        "      if not token.is_alpha:\n",
        "        continue\n",
        "        \n",
        "      # Stopword removal\n",
        "      if token.is_stop:\n",
        "        continue\n",
        "        \n",
        "      # Lemmatization\n",
        "      token = token.lemma_\n",
        "        \n",
        "      # Case folding\n",
        "      token = token.casefold()\n",
        "\n",
        "      # Yield token\n",
        "      yield token"
      ],
      "metadata": {
        "id": "aC-SMwDLtM0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self):\n",
        "    self.tokenizer = TweetTokenizer()\n",
        "    self.lemmatizer = WordNetLemmatizer()\n",
        "    self.stop_words = stopwords.words('english')\n",
        "  \n",
        "  def fit(self, X, y=None):\n",
        "    return self\n",
        "  \n",
        "  def transform(self, X, y=None) -> list:\n",
        "    return [' '.join(self.process_document(doc)) for doc in X]\n",
        "  \n",
        "  def process_document(self, doc: str) -> list:\n",
        "    for token in self.tokenizer.tokenize(doc):\n",
        "      word = token.casefold()\n",
        "      if word not in self.stop_words:\n",
        "        tag = NLTKPreprocessor.wordnet_pos_tag(word)\n",
        "        lemma = self.lemmatizer.lemmatize(word, pos=tag)\n",
        "        yield lemma\n",
        "  \n",
        "  # This is a common method which is widely used across the NLP community\n",
        "  @staticmethod\n",
        "  def wordnet_pos_tag(token: str) -> Generator[str, None, None]:\n",
        "    \"\"\"\n",
        "    Maps POS tags to the first character lemmatize() accepts.\n",
        "    WordNet groups [N]ouns, [V]erbs, [A]djectives, and Adve[R]bs into synsets.\n",
        "    \"\"\"\n",
        "    tag_dict = {\n",
        "        'J': wordnet.ADJ,\n",
        "        'N': wordnet.NOUN,\n",
        "        'V': wordnet.VERB,\n",
        "        'R': wordnet.ADV\n",
        "    }\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "MEJy9zWDBNcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(\n",
        "    lib: str = \"\", \n",
        "    vec_norm: str = 'l2',\n",
        "    n_grams: tuple = (1, 1), \n",
        "    k_best: int = 1000\n",
        "  ) -> Pipeline:\n",
        "\n",
        "  # Use SpaCy to preprocess text\n",
        "  if lib == 'spacy':\n",
        "    count_vectorizer = CountVectorizer(\n",
        "        ngram_range=n_grams,\n",
        "        lowercase=False\n",
        "    )\n",
        "    steps = [\n",
        "      ('spacy_preprocessor', SpacyPreprocessor()),\n",
        "      ('count_vectorizer', count_vectorizer),\n",
        "    ]\n",
        "\n",
        "  \n",
        "  # Use NLTK to preprocess text\n",
        "  elif lib == 'nltk':\n",
        "    count_vectorizer = CountVectorizer(\n",
        "        ngram_range=n_grams,\n",
        "        lowercase=False\n",
        "    )\n",
        "    steps = [\n",
        "      ('nltk_preprocessor', NLTKPreprocessor()),\n",
        "      ('count_vectorizer', count_vectorizer),\n",
        "    ]\n",
        "\n",
        "  # Use CountVectorizer's in-built preprocessing\n",
        "  else:\n",
        "    count_vectorizer = CountVectorizer(\n",
        "        ngram_range=n_grams,\n",
        "        stop_words=stopwords.words('english'), \n",
        "        lowercase=True\n",
        "    )\n",
        "    steps = [('count_vectorizer', count_vectorizer)]\n",
        "\n",
        "  # Customize pipeline\n",
        "  pipe = Pipeline([\n",
        "    *steps,\n",
        "    ('chi2score', SelectKBest(chi2, k=k_best)),\n",
        "    ('tfidf_transformer', TfidfTransformer(norm=vec_norm, use_idf=True)),\n",
        "  ])\n",
        "\n",
        "  # Return custom pipeline\n",
        "  return pipe"
      ],
      "metadata": {
        "id": "-DzUm5HKvtgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multinomial Naive Bayes"
      ],
      "metadata": {
        "id": "gc24Eo6VUzYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_nb = Pipeline([\n",
        "  *preprocess_text(lib='spacy').steps, \n",
        "  ('naive_bayes', MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "uy9-l_GT8txD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_nb = {\n",
        "    'count_vectorizer__ngram_range': [(1, 2), (1, 3)],\n",
        "    'chi2score__k': [500, 1000],\n",
        "    'tfidf_transformer__norm': ['l1', 'l2'],\n",
        "    'naive_bayes__alpha': [0.25, 0.5, 1],\n",
        "}\n",
        "grid_search_nb = GridSearchCV(pipe_nb, param_grid_nb)"
      ],
      "metadata": {
        "id": "JNfgaMxDgayg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_nb.fit(X_train, y_train)\n",
        "print(\n",
        "  'Naive Bayes score: {:.2f}\\nPipeline parameters: {}'\n",
        "  .format(grid_search_nb.best_score_, grid_search_nb.best_params_)\n",
        ")"
      ],
      "metadata": {
        "id": "ZAD0AKH_goc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99dcd159-d5a6-4b59-baa7-a602415b997e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes score: 0.73\n",
            "Pipeline parameters: {'chi2score__k': 500, 'count_vectorizer__ngram_range': (1, 2), 'naive_bayes__alpha': 0.5, 'tfidf_transformer__norm': 'l2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "naive_bayes_pipe = grid_search_nb.best_estimator_"
      ],
      "metadata": {
        "id": "Cf6RXLRziqD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Classification (SVC)"
      ],
      "metadata": {
        "id": "jE72st5YIT_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_svc = Pipeline([\n",
        "  *preprocess_text(lib='spacy').steps, \n",
        "  ('svc', SVC())\n",
        "])"
      ],
      "metadata": {
        "id": "-L217s0aMeeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_svc = {\n",
        "    'count_vectorizer__ngram_range': [(1, 2), (1, 3)],\n",
        "    'chi2score__k': [500, 1000],\n",
        "    # The strength of the regularization is inversely proportional to C. \n",
        "    'svc__kernel': ['linear', 'poly'],\n",
        "    # The penalty is a squared l2 penalty.\n",
        "    'tfidf_transformer__norm': ['l2'], \n",
        "}\n",
        "grid_search_svc = GridSearchCV(pipe_svc, param_grid_svc)"
      ],
      "metadata": {
        "id": "d7SILiV0q2zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_svc.fit(X_train, y_train)\n",
        "print(\n",
        "  'Naive Bayes score: {:.2f}\\nPipeline parameters: {}'\n",
        "  .format(grid_search_svc.best_score_, grid_search_svc.best_params_)\n",
        ")"
      ],
      "metadata": {
        "id": "oGjs_KHaA091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d19e240d-ccb7-4f93-f61a-fcc3b9dcc301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes score: 0.75\n",
            "Pipeline parameters: {'chi2score__k': 1000, 'count_vectorizer__ngram_range': (1, 2), 'svc__kernel': 'linear', 'tfidf_transformer__norm': 'l2'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svc_pipe = grid_search_svc.best_estimator_"
      ],
      "metadata": {
        "id": "IRQ7rYPho-fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Productionizing a trained sentiment analyzer\n"
      ],
      "metadata": {
        "id": "4bx8m4-Wugw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pipe(pipe: Pipeline, path: str):\n",
        "  with open(path, 'wb') as f_out:\n",
        "    pickle.dump(pipe, f_out)\n",
        "\n",
        "def load_pipe(path) -> Pipeline:\n",
        "  with open(path, 'rb') as f_in:\n",
        "    return pickle.load(f_in)"
      ],
      "metadata": {
        "id": "-NXMHkN8uiN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_MODEL = False\n",
        "SAVE_MODEL = False\n",
        "MODEL_PATH = r'/content/drive/MyDrive/sentiment_classifier.pickle'"
      ],
      "metadata": {
        "id": "-5rO6IC4rSNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "  pipe = load_pipe(MODEL_PATH)\n",
        "\n",
        "else:\n",
        "  # Build the model\n",
        "  pipe = Pipeline([\n",
        "    *preprocess_text(lib='spacy', k_best=1000, n_grams=(1, 2), vec_norm='l2').steps, \n",
        "    ('svc', SVC(kernel='linear'))\n",
        "  ])\n",
        "\n",
        "  # Fit the model\n",
        "  pipe.fit(X_train, y_train)\n",
        "\n",
        "  if SAVE_MODEL:\n",
        "    save_pipe(pipe, MODEL_PATH)"
      ],
      "metadata": {
        "id": "0WaRselrqUpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom sentiment classifier in action"
      ],
      "metadata": {
        "id": "pVdhN9X5s4n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(pipe: Pipeline, doc: str):\n",
        "  y_pred = pipe.predict([doc])[0]\n",
        "  sentiments = {\n",
        "      -1: \"Negative\",\n",
        "      0: \"Neutral\",\n",
        "      1: \"Positive\"\n",
        "  }\n",
        "  return sentiments[y_pred]"
      ],
      "metadata": {
        "id": "Eqai-CTtvANq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emojis = {\n",
        "    \"Negative\": \"ðŸ™\",\n",
        "    \"Neutral\": \"ðŸ˜\",\n",
        "    \"Positive\": \"ðŸ™‚\",\n",
        "}\n",
        "for i in np.random.randint(low=0, high=X_test.shape[0], size=(5,)):\n",
        "  doc = X_test.iloc[i]\n",
        "  sentiment = predict_sentiment(pipe, doc)\n",
        "  print(f'[{emojis[sentiment]}] {doc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAYoLbmgvlNl",
        "outputId": "f202c2ca-6f21-4104-e45b-70ea2adc411b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ðŸ™] Earnings per share EPS in 2005 decreased to EUR0 .66 from EUR1 .15 in 2004 .\n",
            "[ðŸ˜] The percentages of shares and voting rights have been calculated in proportion to the total number of shares registered with the Trade Register and the total number of voting rights related to them .\n",
            "[ðŸ˜] The company had earlier said that it was considering different strategic options for the struggling low-cost mobile operator , including a divestment of its holding .\n",
            "[ðŸ˜] Nokia is requesting that the companies stop making and selling the mobile phones and pay monetary damages and costs .\n",
            "[ðŸ˜] The total value of the deliveries is some EUR65m .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "There are some **important things** to consider while creating and deploying the **sentiment analyzer**:\n",
        "* The **training data** should be **consistent with the objective** of the **sentiment analyzer**. \n",
        "> **Don't train the model** using **movie reviews** if the **objective** is to predict\n",
        "the **sentiment** of **financial news articles**.\n",
        "\n",
        "* **Accurately labeling** the **training data** is **critical** for the model to perform well. \n",
        "> If you are creating a **real-world application**, you will have to **spend time labeling the training documents**, unless you use **pre-labeled dataset** Typically, **labeling** should be done by someone with a **good understanding of industry jargon**.\n",
        "\n",
        "* **Sourcing training data** is a **difficult task**. You can use tools such as **web scraping**\n",
        "or **social media scraping**, subject to permissions\n",
        "> Effort should be spent on **sourcing data** from **multiple platforms** and you **shouldn't rely too much on a particular source**.\n",
        "\n",
        "* **Evaluate the performance of your model regularly** and **retrain the model if\n",
        "required**."
      ],
      "metadata": {
        "id": "Fxnm4b7tyQoM"
      }
    }
  ]
}